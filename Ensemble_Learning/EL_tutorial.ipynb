{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: ensemble.py\n",
    "\n",
    "First, import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ensemble import AdaboostTrees\n",
    "from ensemble import BaggedTrees\n",
    "from ensemble import RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is whether someone will play tennis based on weather factors: outlook, temperature, humidity, and wind. These are all categorical variables. The label is binary. Below the dataset is created, as well as the dictionary of possible values each feature can take. Both are required to fit a decision tree. Although it is the same dataset used in the Decision Tree tutorial, the binary label is now {-1, 1} instead of {0, 1} due to the implementation requirements of each ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([['S', 'H', 'H', 'W', -1],\n",
    "                ['S', 'H', 'H', 'S', -1],\n",
    "                ['O', 'H', 'H', 'W', 1],\n",
    "                ['R', 'M', 'H', 'W', 1],\n",
    "                ['R', 'C', 'N', 'W', 1],\n",
    "                ['R', 'C', 'N', 'S', -1],\n",
    "                ['O', 'C', 'N', 'S', 1],\n",
    "                ['S', 'M', 'H', 'W', -1],\n",
    "                ['S', 'C', 'N', 'W', 1],\n",
    "                ['R', 'M', 'N', 'W', 1],\n",
    "                ['S', 'M', 'N', 'S', 1],\n",
    "                ['O', 'M', 'H', 'S', 1],\n",
    "                ['O', 'H', 'N', 'W', 1],\n",
    "                ['R', 'M', 'H', 'S', -1]])\n",
    "\n",
    "Attrs = {\n",
    "    \"Outlook\"       : [\"S\", \"O\", \"R\"],\n",
    "    \"Temperature\"   : [\"H\", \"M\", \"C\"],\n",
    "    \"Humidity\"      : [\"H\", \"N\", \"L\"],\n",
    "    \"Wind\"          : [\"S\", \"W\"]\n",
    "}\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the required inputs to construct a decision tree, we can create our ensembles. The algorithms implemented are Adaboost Trees, Bagged Trees, and Random Forests. The practical differences for each model are the definition of the classifier and the perturbation of the original dataset. So, tuning the hyperparameters is a matter of defining the classifier, or in this case, designing the decision tree classifier used as each individual model in the ensemble.\n",
    "\n",
    "First, we define a model as 10 boosted decision stumps, the classifier and ensemble size as the default values. With this model, the predictions are computed and compared to the observed labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Decision Stumps Train Error: 28.57%\n"
     ]
    }
   ],
   "source": [
    "model = AdaboostTrees()\n",
    "\n",
    "model.fit(X, y, Attrs)\n",
    "\n",
    "preds = model.predict(X, Attrs)\n",
    "\n",
    "train_error = 1 - np.mean(preds == y)\n",
    "\n",
    "print(f\"Adaboost Decision Stumps Train Error: {round(100 * train_error, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a bagged trees model. Defining the model and fitting/predicting is exactly the same as before. This time, we will use an ensemble size of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged Decision Trees Train Error: 14.29%\n"
     ]
    }
   ],
   "source": [
    "model = BaggedTrees(n_classifiers=20)\n",
    "\n",
    "model.fit(X, y, Attrs)\n",
    "\n",
    "preds = model.predict(X, Attrs)\n",
    "\n",
    "train_error = 1 - np.mean(preds == y)\n",
    "\n",
    "print(f\"Bagged Decision Trees Train Error: {round(100 * train_error, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a random forest model. Again, defining and using the model is the same as the other ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Error: 14.29%\n"
     ]
    }
   ],
   "source": [
    "model = RandomForest()\n",
    "\n",
    "model.fit(X, y, Attrs)\n",
    "\n",
    "preds = model.predict(X, Attrs)\n",
    "\n",
    "train_error = 1 - np.mean(preds == y)\n",
    "\n",
    "print(f\"Random Forest Train Error: {round(100 * train_error, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the the tutorial for ensemble.py."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
