{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: decisiontree.py\n",
    "First, we import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import decisiontree as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is whether someone will play tennis based on weather factors: outlook, temperature, humidity, and wind. These are all categorical variables. The label is binary. Below the dataset is created, as well as the dictionary of possible values each feature can take. Both are required to fit a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([['S', 'H', 'H', 'W', 0],\n",
    "                ['S', 'H', 'H', 'S', 0],\n",
    "                ['O', 'H', 'H', 'W', 1],\n",
    "                ['R', 'M', 'H', 'W', 1],\n",
    "                ['R', 'C', 'N', 'W', 1],\n",
    "                ['R', 'C', 'N', 'S', 0],\n",
    "                ['O', 'C', 'N', 'S', 1],\n",
    "                ['S', 'M', 'H', 'W', 0],\n",
    "                ['S', 'C', 'N', 'W', 1],\n",
    "                ['R', 'M', 'N', 'W', 1],\n",
    "                ['S', 'M', 'N', 'S', 1],\n",
    "                ['O', 'M', 'H', 'S', 1],\n",
    "                ['O', 'H', 'N', 'W', 1],\n",
    "                ['R', 'M', 'H', 'S', 0]])\n",
    "\n",
    "Attrs = {\n",
    "    \"Outlook\"       : [\"S\", \"O\", \"R\"],\n",
    "    \"Temperature\"   : [\"H\", \"M\", \"C\"],\n",
    "    \"Humidity\"      : [\"H\", \"N\", \"L\"],\n",
    "    \"Wind\"          : [\"S\", \"W\"]\n",
    "}\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the input data, label vector, and attribute dictionary, the model can be defined and we can train a tree. Here, I define a decision tree using Gini Index to measure the best split, and limit the depth to 1, which is a decision stump. Using the fit function on the defined model, the tree is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dt.DecisionTree(split_metric=\"gini\", max_depth=1)\n",
    "model.fit(X, y, Attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tree is constructed, there are a few things we can do. First, we use the display_links() and display_tree() functions to create a visualization by hand, given that I have not implemented a direct visualization function. With display_links, we can create the general tree structure of links and nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 2\n",
      "Node 2 is a leaf\n",
      "1 -> 3\n",
      "Node 3 is a leaf\n",
      "1 -> 4\n",
      "Node 4 is a leaf\n"
     ]
    }
   ],
   "source": [
    "model.display_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, with display_tree(), we can use the output information to fill in the attributes, rules, and labels for each node/link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Details:\n",
      "ID: 1\n",
      "Depth: 0\n",
      "Attribute: Temperature\n",
      "Rule: None\n",
      "label: None\n",
      "\n",
      "Node Details:\n",
      "ID: 2\n",
      "Depth: 1\n",
      "Attribute: None\n",
      "Rule: H\n",
      "label: 0\n",
      "\n",
      "Node Details:\n",
      "ID: 3\n",
      "Depth: 1\n",
      "Attribute: None\n",
      "Rule: M\n",
      "label: 1\n",
      "\n",
      "Node Details:\n",
      "ID: 4\n",
      "Depth: 1\n",
      "Attribute: None\n",
      "Rule: C\n",
      "label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.display_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the constructed tree:\n",
    "\n",
    "\n",
    "<img src=\"tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this tree to make predictions. Here, we compute predictions on the same dataset and calculate the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Stump Training Error: 35.71%\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X, Attrs)\n",
    "\n",
    "train_error = 1 - np.mean(preds == y)\n",
    "\n",
    "print(f\"Decision Stump Training Error: {round(100 * (train_error), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate is quite high for such a small dataset, so next we construct a full tree and calculate the error rate. This is done by omitting the max_depth hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Training Error: 0.0%\n"
     ]
    }
   ],
   "source": [
    "model = dt.DecisionTree(split_metric='gini')\n",
    "model.fit(X, y, Attrs)\n",
    "\n",
    "preds = model.predict(X, Attrs)\n",
    "\n",
    "train_error = 1 - np.mean(preds == y)\n",
    "\n",
    "print(f\"Full Tree Training Error: {round(100 * (train_error), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the tutorial for decisiontree.py. You may notice the other DecisionTree arguments (\"rand_tree\", \"n_rand_attrs\") were omitted in this introduction. For single trees, these are less practical. These options are required to support random forests in the ensemble methods section, and will be utilized there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
